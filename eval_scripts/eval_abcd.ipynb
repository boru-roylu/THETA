{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import copy\n",
    "import datasets\n",
    "import decouple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers as tfs\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "import analyze_utils\n",
    "import data_collator\n",
    "import eval_utils\n",
    "import input_utils\n",
    "import modeling_bert\n",
    "import utils\n",
    "\n",
    "SCRATCH_DIR = decouple.config('SCRATCH_PARENT_DIR')\n",
    "NFS_DIR = decouple.config('NFS_PARENT_DIR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loads data and initialize paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'abcd'\n",
    "model_path = f'{NFS_DIR}/exp/abcd/finetune/theta-cls-hibert-absolute-pos_init_from_bert-base-uncased-abcd-wwm_job_name-abcd-final-version/abcd-cluster-60-state-12/seed-9/checkpoint-2012/fp32'\n",
    "target_split = 'test' \n",
    "batch_size = 64\n",
    "cuda_device = 0\n",
    "\n",
    "# Structure\n",
    "num_clusters = 60\n",
    "num_splits = 9\n",
    "num_states = 12\n",
    "num_merging = 0\n",
    "\n",
    "fp16 = True\n",
    "debug_mode = False\n",
    "task_name = 'finetune'\n",
    "pos_type = 'absolute'\n",
    "embedding_name = 'bert_mean_pooler_output'\n",
    "model_name = 'theta-cls-hibert'\n",
    "data_config_path = f'../config/data/{data_name}.yaml'\n",
    "coordinator_config_path = f'../config/model/theta-cls-hibert/cls-cluster-state-structure-hibert-absolute-pos-config-1layer-2head.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = utils.read_yaml(data_config_path)\n",
    "\n",
    "eval_fn = eval_utils.get_classification_finetuning_post_process_function(\n",
    "    data_config['config']['num_labels'])\n",
    "\n",
    "pop_keys = ['mlm_labels', 'labels', 'sentence_masked_idxs']\n",
    "\n",
    "assert os.path.isdir(model_path), model_path\n",
    "emb_dir = data_config['path']['embedding_dir'].format(nfs_dir=NFS_DIR)\n",
    "assignment_dir = data_config['path']['assignment_dir'].format(nfs_dir=NFS_DIR)\n",
    "assignment_dir = os.path.join(\n",
    "    assignment_dir, f'{embedding_name}/num_clusters_{num_clusters}')\n",
    "\n",
    "os.makedirs(emb_dir, exist_ok=True)\n",
    "os.makedirs(assignment_dir, exist_ok=True)\n",
    "\n",
    "print(f'{model_path = }')\n",
    "print(f'{emb_dir = }')\n",
    "print(f'{assignment_dir = }')\n",
    "\n",
    "tokenizer = tfs.AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model_config = tfs.AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "dataset_dir = data_config['path']['dataset_dir'].format(scratch_dir=SCRATCH_DIR)\n",
    "\n",
    "columns = [\n",
    "    'input_ids',\n",
    "    'attention_mask',\n",
    "    'num_turns',\n",
    "    data_config['config']['label_name']]\n",
    "\n",
    "local_ds_dir = \\\n",
    "    utils.get_dataset_dir_map(task_name, dataset_dir, model_path, debug_mode)\n",
    "\n",
    "raw_ds = datasets.DatasetDict.load_from_disk(local_ds_dir['raw_ds'])\n",
    "\n",
    "raw_ds = input_utils.add_assignments(raw_ds, assignment_dir, 'cluster')\n",
    "columns.append('cluster_input_ids')\n",
    "columns.append('cluster_attention_mask')\n",
    "\n",
    "state_assignment_dir = \\\n",
    "    data_config['path']['state_assignment_dir'].format(nfs_dir=NFS_DIR)\n",
    "state_assignment_dir = os.path.join(\n",
    "    state_assignment_dir,\n",
    "    f'{embedding_name}/num_clusters_{num_clusters}/num_splits_{num_splits}_num_states_{num_states}_num_merging_{num_merging}')\n",
    "raw_ds = input_utils.add_assignments(raw_ds, state_assignment_dir, 'state')\n",
    "columns.append('state_input_ids')\n",
    "columns.append('state_attention_mask')\n",
    "\n",
    "ds = copy.deepcopy(raw_ds)\n",
    "ds.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shows an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "for turn in raw_ds['train']['dialogue'][idx]:\n",
    "    utt = turn['turn']\n",
    "    party = turn['party']\n",
    "    print(f'{party:>10}: {utt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hierarchical_models as hm\n",
    "\n",
    "coordinator_config = tfs.AutoConfig.from_pretrained(\n",
    "    coordinator_config_path,\n",
    "    use_cache=False,\n",
    "    num_labels=data_config['config']['num_labels'])\n",
    "\n",
    "use_state_sequence_classifier = \\\n",
    "    getattr(coordinator_config, 'use_state_sequence_classifier', False)\n",
    "\n",
    "use_cluster_sequence_classifier = \\\n",
    "    getattr(coordinator_config, 'use_cluster_sequence_classifier', False)\n",
    "\n",
    "if use_cluster_sequence_classifier:\n",
    "    num_clusters = num_clusters * 2\n",
    "else:\n",
    "    num_clusters = None\n",
    "if use_state_sequence_classifier:\n",
    "    num_states = num_states\n",
    "else:\n",
    "    num_states = None\n",
    "\n",
    "model = hm.HierarchicalBertModelForConversationClassification.from_pretrained(\n",
    "    model_path,\n",
    "    coordinator_config=coordinator_config,\n",
    "    num_states=num_states,\n",
    "    num_clusters=num_clusters,\n",
    "    use_state_sequence_classifier=use_state_sequence_classifier,\n",
    "    use_cluster_sequence_classifier=use_cluster_sequence_classifier,\n",
    "    state_sequence_encoder_type='transformer',\n",
    "    cluster_sequence_encoder_type='transformer',\n",
    "    num_labels=data_config['config']['num_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = data_collator.ConversationDataCollator(\n",
    "    data_config['config']['label_name'], tokenizer.pad_token_id)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    ds[target_split], batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loads the model and extract embeddings.\n",
    "You can skips if you already extracted embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda(cuda_device)\n",
    "_ = model.eval()\n",
    "if fp16:\n",
    "    model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels =[]\n",
    "all_logits = []\n",
    "for batch in tqdm.tqdm(data_loader):\n",
    "    batch.pop('num_turns')\n",
    "    utils.obj_to_device(batch, device=cuda_device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**batch)\n",
    "        all_logits.append(output['logits'].cpu().float())\n",
    "        all_labels.append(batch['labels'].cpu())\n",
    "all_logits = torch.cat(all_logits).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "predictions = {'logits': all_logits}\n",
    "inputs = {'labels': all_labels}\n",
    "results = {k: round(v, 3) for k, v in eval_fn(inputs, None, predictions, None, None).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 13:49:34) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d894f953985df1b1fa232bf092ab4758eff604d12535f541d8fc07db67b454f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
